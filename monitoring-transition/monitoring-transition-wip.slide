Monitoring transition

Saurabh Hirani
@sphirani
saurabh.hirani@gmail.com

* 

.image images/sardonic-server-switch-off-phones.png 

* Problem

- What does it take to move from manual to automated to distributed monitoring?

* What this talk is about

- Questions to ask yourself if you monitor manually
- Where to start, how to measure progress, how to cutover?
- Generic learnings with demos you can apply today

* What this talk is NOT about

- X vs Y
- Yet another Nagios rant
- Yet another automated monitoring benefits hand waving

* Infrastructure in context

- AWS
- VMware
- Network devices
- Third party services
- All of the above in multiple environments

* Pre-nirvana setup

- One big, bad nagios host monitoring everything
- Thousands of hosts, tens of thousands of checks
- Hosts, hostgroups, checks, scripts added manually
- Configs scattered through the filesystem
- No self service, blocked by the in-house scapegoat

* Face it

- It works because...

.image images/sardonic-server-users.png

* The price

- How do I add 100 new hosts in an environment?
- How do I get to know about decomissioned hosts?
- How do I monitor dynamic environments - AWS autoscaling groups?
- We added 2 new switches - are they monitored?
- How do I build tooling around it?

* Why don't you just...

- Use chef, puppet, etc. to pull out all the information?
- Use a CMDB?
- Use X tool? - it's so nice and shiny

* The hard truth

- Realize that monsters are not born - they are created
- Buckle up and clean up the mess
- Finish what you start

* Better said than done

- Monitoring systems don't have a downtime
- Transparent cutover expected
- Show measurable progress

* What you are really solving

- Audit - Is everything "monitorable"?
- Fix what you can - clean AWS tags, chef roles, etc.
- Zoom out and see patterns
- Rinse, repeat == everything is monitorable

* Revisit your monitoring strategy

- Monitor hosts where X == Y
- Equally important ignore hosts where A == B
- Pull data from multiple DBs (Chef, AWS, etc.)
- Different levels of grouping - location, functionality, product, etc.
- Choice: icinga2

* Where do I start?

- Take stock
- Demo: inframer, nagios-api, nagira

* Everything is not everywhere

- Config management systems - chef, puppet, etc
- Cloud providers - AWS, VMWare, etc
- CMDB prelimnary population - Device42
- Snowflakes
- Demo: icinga2 multi DB collection

* Fix legacy check scripts

- Microservices? 
- Uniform URIs!
- HTTP status codes!
- One life is too short to write boilerplate json
- Fix it while you can
- Demo: jsonalyzer

* Consolidate and distribute

- Consolidate server side checks
- Distribute client side checks
- Package your plugins and their configs
- Demo: dockerized fpm

* Buy in on what you can break

- No one has a single qa, stage, prod
- Multiple environments to cutover from
- Target the low hanging fruits

* Homework done till now

- Everything is not everywhere
- Fix legacy check scripts
- Consolidate and distribute
- Buy in on what you can break

* Heads down, execution time
 
- Add, deploy, compare, repeat
- Progressive cutover
- Demo: nagios, icinga2, inframer

* Monitor the monitor

.image images/sardonic-server-resources.png

* Distributed checks

- One master, multiple slaves - icinga2 feature
- If you designed the previous steps correctly, horizontal scalability is a side effect
- Distributed monitoring != Distributed checks

* The fun stuff

- It's fun because you have a solid foundation
- Chat bots integration
- REST API tools - upgrades, downtimes, ASG monitoring
.link https://github.com/saurabh-hirani/icinga2-api-examples icinga2-api-examples
.link https://github.com/saurabh-hirani/icinga2_api icinga2_api

* Conclusion

- Cleanup is insightful
- Automate for the maintainers 
- Catch yourself cheating
- The rent is due everyday

* When are you starting?

  - “All happy families are alike; each unhappy family is unhappy in its own way.”
    Leo Tolstoy

- Catch me to talk about your manual monitoring pain points
- Questions?
